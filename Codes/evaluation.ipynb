{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe71557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('RAFT')\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "from utils import flow_viz\n",
    "from utils import frame_utils\n",
    "\n",
    "from raft import RAFT\n",
    "from utils.utils import InputPadder, forward_interpolate\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import copy\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da41e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseTransformer2(nn.Module):\n",
    "    def __init__(self, num_layers=12, embed_dim=512, num_heads=8, ff_dim=512, \n",
    "                 dropout=0.1, out_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 361, embed_dim))  # 19x19=361 tokens\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-layer normalization\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # Output MLP for pose regression\n",
    "        self.fc1 = nn.Linear(embed_dim, out_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = x.view(B, self.embed_dim, -1).permute(0, 2, 1)  # (B, 361, embed_dim)\n",
    "        # x = x + self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "728e48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseTransformer(nn.Module):\n",
    "    def __init__(self, in_dim=2, num_layers=12, embed_dim=512, num_heads=8, ff_dim=512, \n",
    "                 dropout=0.1, out_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.pos_embedding = nn.Parameter(torch.randn(1, 361, embed_dim))  # 19x19=361 tokens\n",
    "        self.fc0 = nn.Linear(in_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-layer normalization\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # Output MLP for pose regression\n",
    "        self.fc1 = nn.Linear(embed_dim, out_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        B = x.shape[0]\n",
    "        x = x.view(B, self.embed_dim, -1).permute(0, 2, 1)  # (B, 361, embed_dim)\n",
    "        # x = x + self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0df272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModResNet2(nn.Module):\n",
    "    def __init__(self, in_chans, out):\n",
    "        super(ModResNet2, self).__init__()\n",
    "        original_model = models.resnet101(pretrained=True)\n",
    "        original_model.conv1 = nn.Conv2d(\n",
    "                    in_channels=in_chans,  # Change from 3 to 1 to accept grayscale images\n",
    "                    out_channels=original_model.conv1.out_channels,\n",
    "                    kernel_size=original_model.conv1.kernel_size,\n",
    "                    stride=original_model.conv1.stride,\n",
    "                    padding=original_model.conv1.padding,\n",
    "                    bias=original_model.conv1.bias)\n",
    "        self.features = nn.Sequential(\n",
    "            original_model.conv1,\n",
    "            original_model.bn1,\n",
    "            original_model.relu,\n",
    "            original_model.maxpool,\n",
    "            original_model.layer1,\n",
    "            original_model.layer2,\n",
    "            original_model.layer3,\n",
    "            original_model.layer4\n",
    "        )\n",
    "        self.avgpool = original_model.avgpool\n",
    "        num_features = original_model.fc.in_features\n",
    "        num_out_feas = out\n",
    "        original_model.fc = nn.Linear(num_features, num_out_feas)\n",
    "        self.fc = original_model.fc\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        out_fc = self.fc(x)\n",
    "        return out_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "450e2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiamesePoseNet3b_trans2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiamesePoseNet3b_trans2d, self).__init__()\n",
    "        self.model = ModResNet2(1,512)\n",
    "        self.model2a = PoseTransformer2(embed_dim=512, out_dim=2, num_layers=2)\n",
    "        self.model2b = PoseTransformer2(embed_dim=512, out_dim=2, num_layers=2)\n",
    "        self.model2d = PoseTransformer2(embed_dim=512, out_dim=4, num_layers=2)\n",
    "    def forward(self, rgbd1):\n",
    "        f1_rgb = self.model(rgbd1[:,0:1,:,:])\n",
    "        f2_rgb = self.model(rgbd1[:,1:,:,:])\n",
    "        pfocal = self.model2a(f1_rgb - f2_rgb)\n",
    "        pcenters = self.model2b(f1_rgb - f2_rgb)\n",
    "        pquat = self.model2d(f1_rgb - f2_rgb)\n",
    "        return pfocal, pcenters, [], pquat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513d8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class homo_opt2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(homo_opt2, self).__init__()\n",
    "        self.model = ModResNet2(1,512)\n",
    "        self.model2e = PoseTransformer(in_dim=2, out_dim=512, num_layers=2) #2\n",
    "        self.model2f = PoseTransformer(in_dim=512, out_dim=2, num_layers=2)#8\n",
    "    def forward(self, rgbd1, uvc=None):\n",
    "        f1_rgb = self.model(rgbd1[:,0:1,:,:])\n",
    "        f2_rgb = self.model(rgbd1[:,1:,:,:])\n",
    "        \n",
    "        # fcomb = self.model(rgbd1)\n",
    "        p_all_del_uv = []\n",
    "        if uvc is not None:\n",
    "            for ii in range(len(uvc)):\n",
    "                sh = uvc[ii].shape[0]\n",
    "                Nmax = 20000\n",
    "                rndd = sh//Nmax\n",
    "                inter_p = []\n",
    "                for jj in range(rndd):\n",
    "                    nnmin, nnmax = jj*Nmax, jj*Nmax + Nmax\n",
    "                    f_uv = self.model2e(uvc[ii][:,0:2][nnmin:nnmax,:].cuda())\n",
    "                    p_del_uv_ = self.model2f(f_uv + f1_rgb[ii,:] - f2_rgb[ii,:]) #self.model2f(f_uv + fcomb[ii,:]) #\n",
    "                    inter_p.append(p_del_uv_)\n",
    "                if (sh - rndd*Nmax) > 0:\n",
    "                    nnmin, nnmax = rndd*Nmax, sh\n",
    "                    f_uv = self.model2e(uvc[ii][:,0:2][nnmin:nnmax,:].cuda())\n",
    "                    p_del_uv_ = self.model2f(f_uv + f1_rgb[ii,:] - f2_rgb[ii,:]) #self.model2f(f_uv + fcomb[ii,:]) #\n",
    "                    inter_p.append(p_del_uv_)\n",
    "                \n",
    "                p_del_uv = torch.vstack(inter_p)  \n",
    "                p_all_del_uv.append(p_del_uv)\n",
    "        return p_all_del_uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a490e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0530a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAFT\n",
    "sys.path.append('core')\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', help=\"restore checkpoint\")\n",
    "parser.add_argument('--dataset', help=\"dataset for evaluation\")\n",
    "parser.add_argument('--small', action='store_true', help='use small model')\n",
    "parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')\n",
    "parser.add_argument('--alternate_corr', action='store_true', help='use efficent correlation implementation')\n",
    "args, unknown = parser.parse_known_args()\n",
    "model = torch.nn.DataParallel(RAFT(args))\n",
    "model.load_state_dict(torch.load('./raft-things.pth'))\n",
    "model = model.eval().to(device)\n",
    "mod0 = copy.deepcopy(model).eval()\n",
    "modref = copy.deepcopy(model).eval()\n",
    "modh = SiamesePoseNet3b_trans2d().to(device)\n",
    "modh = modh.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81687b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flow_former\n",
    "import sys\n",
    "sys.path.append('FlowFormer-Official')\n",
    "sys.path.append('core_f')\n",
    "from configs.default import get_cfg\n",
    "from configs.things_eval import get_cfg as get_things_cfg\n",
    "from configs.small_things_eval import get_cfg as get_small_things_cfg\n",
    "from core_f.utils.misc import process_cfg\n",
    "import datasets\n",
    "from core_f.utils import flow_viz\n",
    "from core_f.utils import frame_utils\n",
    "from core_f.FlowFormer import build_flowformer\n",
    "from core_f.utils.utils import InputPadder, forward_interpolate\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', help=\"restore checkpoint\")\n",
    "parser.add_argument('--dataset', help=\"dataset for evaluation\")\n",
    "parser.add_argument('--small', action='store_true', help='use small model')\n",
    "parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')\n",
    "parser.add_argument('--alternate_corr', action='store_true', help='use efficent correlation implementation')\n",
    "args, unknown = parser.parse_known_args()\n",
    "cfg = get_things_cfg()\n",
    "cfg.update(vars(args))\n",
    "cfg.model = \"./things.pth\"\n",
    "model_ff = torch.nn.DataParallel(build_flowformer(cfg))\n",
    "model_ff.load_state_dict(torch.load(cfg.model))\n",
    "model_ff = model_ff.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a20b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_mm = np.array([[508.3997, 0, 316.0652],[0,677.8663,254.0068],[0, 0, 1]])\n",
    "nimg_0 = [13,19,22,21,48,65,52,54,55,31,32,7,14]\n",
    "klb_dir = '/home/thomas/ScoreFunc/smart_bridge2/'\n",
    "IMGS, TRNS, QUTS = [], [], []\n",
    "for idx in nimg_0:\n",
    "    img_idx = cv2.imread(klb_dir+'img'+str(idx)+'.png')\n",
    "    IMGS.append(cv2.cvtColor(img_idx,cv2.COLOR_BGR2RGB)) \n",
    "    try:\n",
    "        trs = np.load(klb_dir+'trans_'+str(idx)+'.npy').flatten()\n",
    "        qts = np.load(klb_dir+'quat_'+str(idx)+'.npy').flatten()\n",
    "        TRNS.append(trs) \n",
    "        QUTS.append(qts)\n",
    "    except:\n",
    "        TRNS.append(None) \n",
    "        QUTS.append(None)   \n",
    "\n",
    "def calc_ffbx(K_mm, Rts, xyz_0, trs, th=100):\n",
    "    zuv0 = K_mm@Rts@(xyz_0.T - trs.reshape(-1,1))\n",
    "    uv_0 = (zuv0[0:2,:]/zuv0[2:,:]).astype(np.int16)\n",
    "    return uv_0\n",
    "\n",
    "BXMs = []\n",
    "n_patches = 4\n",
    "for idx in nimg_0:\n",
    "    trs = np.load(klb_dir+'trans_'+str(idx)+'.npy').flatten()\n",
    "    qts = np.load(klb_dir+'quat_'+str(idx)+'.npy').flatten() #0.95\n",
    "    Rts = np.array(R.from_quat([qts[1],qts[2],qts[3],qts[0]]).as_matrix())\n",
    "    xyz_0 = np.array([[-1.03, -10.92, 16.96],[-1.03,2.85,16.96],[-1.03,2.85,13.97],[-1.03,-10.66,13.97]])\n",
    "    xyz_1 = np.array([[-1.03, 2.85, 16.96],[-1.03,16.62,16.96],[-1.03,16.36,13.97],[-1.03,2.85,13.97]])\n",
    "    xyz_2 = np.array([[-1.03, 2.85, 13.97],[-1.03,16.36,13.97],[-1.03,16.1,10.97],[-1.03,2.85,10.97]])\n",
    "    xyz_3 = np.array([[-1.03,-10.66, 13.97],[-1.03,2.85,13.97],[-1.03,2.85,10.97],[-1.03,-10.39,10.97]])\n",
    "    bxm = []\n",
    "    uv_0 = calc_ffbx(K_mm, Rts, xyz_0, trs)\n",
    "    uv_1 = calc_ffbx(K_mm, Rts, xyz_1, trs)\n",
    "    uv_2 = calc_ffbx(K_mm, Rts, xyz_2, trs)\n",
    "    uv_3 = calc_ffbx(K_mm, Rts, xyz_3, trs)\n",
    "    BXMs.append([uv_0.T, uv_1.T, uv_2.T, uv_3.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ca176b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_mskk(im, uv_1):\n",
    "    im = np.array(im).astype(np.uint8)\n",
    "    hh,ww = im.shape[0:2]\n",
    "    msk = np.zeros((hh,ww),dtype=np.uint8)\n",
    "    pts = np.array(uv_1[0:2,:]).T.astype(np.int32)\n",
    "    cv2.fillPoly(msk, pts[np.newaxis,:,:], 1)\n",
    "    if len(im.shape)>2:\n",
    "        msk = msk[:,:,None]\n",
    "    imgi = msk*im\n",
    "    return imgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa203d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_homo(img1, Hp_1_2, ww=640, hh=480, mxlim=3500, inv=False):\n",
    "    H, W = img1.shape[:2]\n",
    "    corners = np.array([\n",
    "        [0, 0, 1],\n",
    "        [W-1, 0, 1],\n",
    "        [W-1, H-1, 1],\n",
    "        [0, H-1, 1]\n",
    "    ], dtype=np.float32).T\n",
    "\n",
    "    warped = Hp_1_2 @ corners\n",
    "    warped /= warped[2]\n",
    "    min_x, min_y = np.min(warped[:2],axis=1)\n",
    "    max_x, max_y = np.max(warped[:2],axis=1)\n",
    "    shift_x = -min_x if min_x < 0 else 0\n",
    "    shift_y = -min_y if min_y < 0 else 0\n",
    "\n",
    "    T = np.array([[1, 0, shift_x],\n",
    "                  [0, 1, shift_y],\n",
    "                  [0, 0, 1]], dtype=np.float32)\n",
    "    H_shifted = T @ Hp_1_2\n",
    "    new_w = max_x + shift_x\n",
    "    new_h = max_y + shift_y\n",
    "    sx = ww / new_w\n",
    "    sy = hh / new_h\n",
    "    S = np.array([[sx, 0, 0],\n",
    "                  [0, sy, 0],\n",
    "                  [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "    H_final = S @ H_shifted\n",
    "    blk_img = cv2.warpPerspective(img1, H_final, (ww, hh))\n",
    "    return blk_img, H_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c707f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_homo(img, K1, K2, R1, R2, R2_, params, inv=False, Hp=None, ori=False, dft=False):\n",
    "    img = np.array(img).astype(np.uint8)\n",
    "    h, w = img.shape[:2]\n",
    "    sf, sc = params\n",
    "    K2 = np.array(K2)\n",
    "    K2[0,0] = sf[0] + K2[0,0]\n",
    "    K2[1,1] = sf[1] + K2[1,1]\n",
    "    K2[0,2] = sc[0] + K2[0,2]\n",
    "    K2[1,2] = sc[1] + K2[1,2]\n",
    "    if inv:\n",
    "        H2 = (K2@R2)@np.linalg.inv(K1@R1)\n",
    "        Hp = H2@Hp\n",
    "        Hinv = np.linalg.inv(Hp)\n",
    "        Hinv /= Hinv[2, 2]\n",
    "        rotated_image = cv2.warpPerspective(img, Hinv, (w, h))\n",
    "        return rotated_image, Hp\n",
    "    else:\n",
    "        H_o = (K2@R2)@np.linalg.inv(K1@R1)\n",
    "        H_o_ = (K2@R2_)@np.linalg.inv(K1@R1)\n",
    "        H = H_o/H_o[2, 2]\n",
    "        if dft:\n",
    "            rotated_image = cv2.warpPerspective(img, H, (w, h))\n",
    "        else:\n",
    "            rotated_image, H_o = remap_homo(img, H)\n",
    "        Hf = H_o \n",
    "        return rotated_image, K2, R2, Hf, H_o_#H_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77080433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fratio(uv_1, uv_2, th=0.2, w=640, h=480):\n",
    "    uv_2[0,uv_2[0,:]<0] = 0\n",
    "    uv_2[0,uv_2[0,:]>w-1] = w-1\n",
    "    uv_2[1,uv_2[1,:]<0] = 0\n",
    "    uv_2[1,uv_2[1,:]>h-1] = h-1\n",
    "    vmin,vmax = np.nanmin(uv_2[1,:])/h,np.nanmax(uv_2[1,:])/h\n",
    "    umin,umax = np.nanmin(uv_2[0,:])/w,np.nanmax(uv_2[0,:])/w\n",
    "    a2 = (vmax-vmin)*(umax-umin)\n",
    "    vmin1,vmax1 = np.nanmin(uv_1[1,:])/h,np.nanmax(uv_1[1,:])/h\n",
    "    umin1,umax1 = np.nanmin(uv_1[0,:])/w,np.nanmax(uv_1[0,:])/w\n",
    "    a1 = (vmax1-vmin1)*(umax1-umin1)\n",
    "    flag = (a2/a1)>=th\n",
    "    if flag:\n",
    "        return True\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a87844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f8b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(image1, uv_, R1_, K_mm, level):\n",
    "    image1 = cv2.resize(image1, (640, 480), interpolation=cv2.INTER_LINEAR)\n",
    "    ru = R.from_matrix(R1_).as_euler('xyz',degrees=True) \n",
    "    K1 = np.array(K_mm)\n",
    "    K2 = np.array(K_mm)\n",
    "    if level==0:\n",
    "        df = np.random.uniform(-20.0,20.0,2)\n",
    "        dc = np.random.uniform(-20.0,20.0,2)\n",
    "        d_ang = np.random.uniform(-5.0,5.0,3)\n",
    "    elif level==1:\n",
    "        df = random.choice([np.random.uniform(-60.0,-20,2), np.random.uniform(20,60,2)])\n",
    "        dc = random.choice([np.random.uniform(-60.0,-20,2), np.random.uniform(20,60,2)])\n",
    "        d_ang = random.choice([np.random.uniform(-15,-5,3), np.random.uniform(5,15,3)])\n",
    "    elif level==2:\n",
    "        df = random.choice([np.random.uniform(-100,-60,2), np.random.uniform(60,100,2)])\n",
    "        dc = random.choice([np.random.uniform(-100,-60,2), np.random.uniform(60,100,2)])\n",
    "        d_ang = random.choice([np.random.uniform(-30,-15,3), np.random.uniform(15,30,3)])\n",
    "\n",
    "    ang_ = np.random.uniform(-3,3)\n",
    "    params = (df, dc)\n",
    "    R2 = R.from_euler('xyz', d_ang+ru, degrees=True).as_matrix()\n",
    "    R2_ = R.from_euler('xyz', d_ang+ru+ang_, degrees=True).as_matrix()\n",
    "    image1 = comp_mskk(image1, uv_.astype(np.int16))\n",
    "    image2, _, _, Hp, Hp_ = construct_homo(image1, K1, K2, R1_, R2, R2_, params, dft=False)\n",
    "    return image1, image2, Hp, Hp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9100086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optical_flow(image1, image2, UV1): #LK optical flow\n",
    "    if image1.ndim == 3:\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    if image2.ndim == 3:\n",
    "        image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(\n",
    "        image1, image2, None,\n",
    "        pyr_scale=0.5, levels=5, winsize=15,\n",
    "        iterations=5, poly_n=7, poly_sigma=1.5,\n",
    "        flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    H, W = flow.shape[:2]\n",
    "    UV1 = UV1.astype(np.float32)\n",
    "    u = UV1[:,0]\n",
    "    v = UV1[:,1]\n",
    "    x0 = np.floor(u).astype(int)\n",
    "    y0 = np.floor(v).astype(int)\n",
    "    x1 = np.clip(x0+1, 0, W-1)\n",
    "    y1 = np.clip(y0+1, 0, H-1)\n",
    "\n",
    "    wx = u - x0\n",
    "    wy = v - y0\n",
    "    dx = (1-wx)*(1-wy)*flow[y0,x0,0] + wx*(1-wy)*flow[y0,x1,0] + \\\n",
    "         (1-wx)*wy*flow[y1,x0,0]     + wx*wy*flow[y1,x1,0]\n",
    "    dy = (1-wx)*(1-wy)*flow[y0,x0,1] + wx*(1-wy)*flow[y0,x1,1] + \\\n",
    "         (1-wx)*wy*flow[y1,x0,1]     + wx*wy*flow[y1,x1,1]\n",
    "    UV2 = np.column_stack([u+dx, v+dy])\n",
    "    return UV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08d181f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_raft_flow(mod, image1, image2, UV1):\n",
    "    with torch.no_grad():\n",
    "        image1 = torch.tensor(image1).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        image2 = torch.tensor(image2).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        flow_low, flow_pr = mod(image1, image2, iters=12, test_mode=True) #iters=12\n",
    "        flow_pr_ = flow_pr.squeeze(0).permute(1, 2, 0)\n",
    "        duv_flow_pr = flow_pr_[UV1[:,1], UV1[:,0],:]\n",
    "        p_UV2 = duv_flow_pr.cpu().numpy() + UV1\n",
    "        return p_UV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98742c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flowf_flow(mod, image1, image2, UV1):\n",
    "    with torch.no_grad():\n",
    "        image1 = torch.tensor(image1).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        image2 = torch.tensor(image2).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "        padder = InputPadder(image1.shape)\n",
    "        image1, image2 = padder.pad(image1, image2)\n",
    "        flow_pre = mod(image1, image2)\n",
    "        flow_pre = padder.unpad(flow_pre[0]).cpu()[0]\n",
    "        flow_pr_ = flow_pre.squeeze(0).permute(1, 2, 0)\n",
    "        duv_flow_pr = flow_pr_[UV1[:,1], UV1[:,0],:]\n",
    "        p_UV2 = duv_flow_pr.cpu().numpy() + UV1\n",
    "        return p_UV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a968c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_k(fs, cs, Kn):\n",
    "    Kn = np.array(Kn)\n",
    "    Kn[0,0] = fs[0] + Kn[0,0]\n",
    "    Kn[1,1] = fs[1] + Kn[1,1]\n",
    "    Kn[0,2] = cs[0] + Kn[0,2]\n",
    "    Kn[1,2] = cs[1] + Kn[1,2]\n",
    "    return Kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f1d2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRfq(qn):\n",
    "    Rm = np.array(R.from_quat([qn[1],qn[2],qn[3],qn[0]]).as_matrix())\n",
    "    return Rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b52cabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homo_unwarp(img, Hn, w=640, h=480):\n",
    "    img = np.array(img).astype(np.uint8)\n",
    "    Hinv = np.linalg.inv(Hn)\n",
    "    Hinv /= Hinv[2, 2]\n",
    "    rotated_image = cv2.warpPerspective(img, Hinv, (w, h))\n",
    "    return rotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5abd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def hog_similarity(img1, img2):\n",
    "    img1 = np.array(img1).astype(np.uint8)\n",
    "    img2 = np.array(img2).astype(np.uint8)\n",
    "    feat1 = hog(img1, pixels_per_cell=(8, 8), cells_per_block=(2, 2),\n",
    "                orientations=9, block_norm='L2-Hys', feature_vector=True)\n",
    "    feat2 = hog(img2, pixels_per_cell=(8, 8), cells_per_block=(2, 2),\n",
    "                orientations=9, block_norm='L2-Hys', feature_vector=True)\n",
    "    sim = cosine_similarity([feat1], [feat2])[0, 0]\n",
    "    sim = (sim + 1) / 2.0\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a40b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homo_align(modhn, img1, img2, Kmm, sz=300, iters=1, Hp=None):\n",
    "    with torch.no_grad():\n",
    "        img1_0 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "        img2_0p = np.array(img2).astype(np.uint8)\n",
    "        img2_0r = np.array(img2).astype(np.uint8)\n",
    "        img2_0 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "        img1_00, img2_00 = np.array(img1_0).astype(np.uint8), np.array(img2_0).astype(np.uint8)\n",
    "        img_tsr10 = F.interpolate(torch.tensor(img1_00/255.0).unsqueeze(0).unsqueeze(0), size=(sz,sz), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "        K1 = np.array(Kmm)\n",
    "        H12_c, H21_c = np.eye(3), np.eye(3)\n",
    "        mtx1 = []\n",
    "        Hs = []\n",
    "        img2_trans = []\n",
    "        if Hp is not None:\n",
    "            img2_0r = homo_unwarp(img2_0p, Hp)\n",
    "            return img2_0r, Hp, None\n",
    "        else:\n",
    "            for j in range(iters):\n",
    "                img2_trans.append(img2_0r.astype(np.uint8))\n",
    "                mtrc10 = hog_similarity(img1_00, img2_0)\n",
    "                mtx1.append(mtrc10)\n",
    "                Hs.append(H12_c)\n",
    "                img_tsr2 = F.interpolate(torch.tensor(img2_0/255.0).unsqueeze(0).unsqueeze(0), size=(sz,sz), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "                comb_tsr1 = torch.stack([img_tsr10, img_tsr2],0).float()\n",
    "                pred_homo1 = modhn(comb_tsr1.unsqueeze(0).to(device))\n",
    "                fscale1, cscale1, _, quat1 = pred_homo1\n",
    "                fs1, cs1  = fscale1.cpu().numpy(), cscale1.cpu().numpy()\n",
    "                qtn1 = quat1/quat1.norm(p=2,dim=1,keepdim=True)\n",
    "                qtn1 = qtn1.cpu().numpy()\n",
    "                K12 = update_k(fs1[0,:], cs1[0,:], K1)\n",
    "                R12 = calRfq(qtn1[0,:].flatten().tolist())\n",
    "                try:\n",
    "                    H12 = K12 @ R12 @ np.linalg.inv(K1)\n",
    "                    H12_c = np.array(H12 @ H12_c)\n",
    "                    img2_0 = homo_unwarp(img2_00, np.array(H12_c))\n",
    "                    img2_0r = homo_unwarp(img2_0p, np.array(H12_c))\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            b_ind = np.argmax(mtx1)\n",
    "            b_sc = np.max(mtx1)\n",
    "            print('b_sc: ', b_sc)\n",
    "            return img2_trans[b_ind], Hs[b_ind], b_sc #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd9d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_pred_flow(p_uv2_0, H12_c):\n",
    "    p_uv2_ = p_uv2_0.T\n",
    "    onsx = np.ones_like(p_uv2_[0,:])\n",
    "    p_uv2_ = np.vstack((p_uv2_,onsx))\n",
    "    p_uv2_ = H12_c@p_uv2_\n",
    "    p_uv2_ = p_uv2_[0:2,:] / (p_uv2_[2:,:] + 1e-12)\n",
    "    p_uv2_ = p_uv2_.T\n",
    "    return p_uv2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adb16f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errs(preds, UV2):\n",
    "    errs = []\n",
    "    for p_ in preds:\n",
    "        errs.append(np.mean(np.abs(p_ - UV2)))\n",
    "    return errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdeec0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_progress3(apt, tit):\n",
    "    clear_output(wait=True)\n",
    "    labs = [\n",
    "        'Pure_Homo', 'LK.', 'RAFT', 'RAFT_FT', 'F.Former',\n",
    "        'LK_RMR', 'RAFT_RMR', 'RAFT_FT_RMR', 'F.Former_RMR'\n",
    "    ]\n",
    "    difficulties = ['Simple', 'Medium', 'Hard']\n",
    "    apt = np.asarray(apt)\n",
    "    assert apt.ndim == 3, \"Expected apt shape (3, 9, num_models)\"\n",
    "    data = apt.mean(axis=1)\n",
    "    num_diff, num_models = data.shape\n",
    "    x = np.arange(num_diff)\n",
    "    bar_width = 0.8 / num_models\n",
    "    plt.figure(figsize=(13, 5))\n",
    "    colors = [\n",
    "        \"#4E79A7\",  # blue\n",
    "        \"#F28E2B\",  # orange\n",
    "        \"#E15759\",  # red\n",
    "        \"#76B7B2\",  # teal\n",
    "        \"#59A14F\",  # green\n",
    "        \"#EDC948\",  # yellow\n",
    "        \"#B07AA1\",  # purple\n",
    "        \"#FF9DA7\",  # pink\n",
    "        \"#9C755F\",  # brown\n",
    "    ]\n",
    "    for i in range(num_models):\n",
    "        xpos = x - 0.4 + i * bar_width\n",
    "        plt.bar(\n",
    "            xpos,\n",
    "            data[:, i],\n",
    "            width=bar_width,\n",
    "            color=colors[i % len(colors)],\n",
    "            label=labs[i]\n",
    "        )\n",
    "        for j in range(num_diff):\n",
    "            plt.text(\n",
    "                xpos[j],\n",
    "                data[j, i],\n",
    "                f\"{data[j, i]:.2f}\",\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=10,\n",
    "                rotation=90\n",
    "            )\n",
    "\n",
    "    plt.xticks(x, difficulties)\n",
    "    plt.xlabel(\"Difficulty Level\")\n",
    "    plt.ylabel(\"Average Pixel Error\")\n",
    "    plt.title(\n",
    "        tit,\n",
    "        color=\"#1f3a5f\",   # deep blue\n",
    "        fontsize=14,\n",
    "        fontweight='semibold'\n",
    "    )\n",
    "    plt.ylim(0, np.max(data) * 1.15)\n",
    "    plt.legend(\n",
    "        title=\"Models\",\n",
    "        bbox_to_anchor=(1.02, 0.95),\n",
    "        loc='upper left',\n",
    "        fontsize=9\n",
    "    )\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "071d4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_models = './PATCH_OPT_MODELS/'\n",
    "homo_models = './PATCH_HOMO_MODELS/'\n",
    "metxs = []\n",
    "for lev in range(3):\n",
    "    print('current_level: ', lev)\n",
    "    agt, apt = [], []\n",
    "    while len(apt)<1000:\n",
    "        idx = np.random.randint(len(nimg_0))\n",
    "        image1 = IMGS[idx]\n",
    "        try:\n",
    "            R1_ = QUTS[idx]\n",
    "            if len(R1_.shape)<2:\n",
    "                R1_ = np.array(R.from_quat([R1_[1],R1_[2],R1_[3],R1_[0]]).as_matrix())\n",
    "        except:\n",
    "             R1_ = np.eye(3)\n",
    "        pz = np.random.randint(len(BXMs[idx]))\n",
    "        modh.load_state_dict(torch.load(homo_models+'/homo_match_comb_' +str(pz)+ '.pth',map_location='cuda:0'))\n",
    "        modh = modh.eval()\n",
    "        modref.load_state_dict(torch.load(opt_models+'/raft_match_comb_' +str(pz)+ '.pth'))\n",
    "        uv_ = BXMs[idx][pz].T\n",
    "        \n",
    "        image1, image2, Hp, Hp_ = gen_data(image1, uv_, R1_, K_mm, level=lev)\n",
    "        ones_ = np.ones_like(uv_[0,:])\n",
    "        uv_1 = np.vstack((uv_,ones_))\n",
    "        uv_2 = Hp @ uv_1\n",
    "        uv_2 = uv_2[0:2,:]/uv_2[2:,:]\n",
    "        flg_ = fratio(np.array(uv_), uv_2, th=0.3)\n",
    "        if flg_ is not None:\n",
    "                comb_imgs = np.hstack((image1,image2))\n",
    "                pts_1 = np.array(uv_.T).astype(np.int32)\n",
    "                msk__1 = np.zeros_like(image1)\n",
    "                cv2.fillPoly(msk__1, pts_1[np.newaxis,:,:], 1)\n",
    "                vv_1, uu_1, _ = np.where(msk__1 == 1)\n",
    "                ones_1 = np.ones_like(vv_1)\n",
    "                uv_op = np.vstack((uu_1, vv_1, ones_1))\n",
    "                uv_2op = Hp @ uv_op\n",
    "                uv_2op = uv_2op[0:2,:]/uv_2op[2:,:]\n",
    "                hhn, wwn = image1.shape[0:2] \n",
    "                viz_flg = (uv_2op[0,:]>=0) * (uv_2op[0,:]<wwn) * (uv_2op[1,:]>=0) * (uv_2op[1,:]<hhn)\n",
    "                UV1 = np.vstack((uu_1[viz_flg],vv_1[viz_flg])).T\n",
    "                UV2 = uv_2op[:,viz_flg].T\n",
    "\n",
    "                pred_UV2_opt = compute_optical_flow(image1, image2, UV1)\n",
    "                pred_UV2_raft = compute_raft_flow(mod0, image1, image2, UV1)\n",
    "                pred_UV2_flowf = compute_flowf_flow(model_ff, image1, image2, UV1)\n",
    "                pred_UV2_raft_ft = compute_raft_flow(modref, image1, image2, UV1)\n",
    "\n",
    "                img2_rect, H12_, _ = homo_align(modh, image1, image2, K_mm, iters=25, Hp=None)\n",
    "                pred_UV2_opt_rect = compute_optical_flow(image1, img2_rect, UV1)\n",
    "                pred_UV2_opt_rect_algn = rect_pred_flow(pred_UV2_opt_rect, H12_)\n",
    "                pred_UV2_raft_rect = compute_raft_flow(mod0, image1, img2_rect, UV1)\n",
    "                pred_UV2_raft_rect_algn = rect_pred_flow(pred_UV2_raft_rect, H12_)\n",
    "\n",
    "                pred_UV2_flowf_rect = compute_flowf_flow(model_ff, image1, img2_rect, UV1)\n",
    "                pred_UV2_flowf_rect_algn = rect_pred_flow(pred_UV2_flowf_rect, H12_)\n",
    "\n",
    "                pred_UV2_raft_ft_rect = compute_raft_flow(modref, image1, img2_rect, UV1)\n",
    "                pred_UV2_raft_ft_rect_algn = rect_pred_flow(pred_UV2_raft_ft_rect, H12_)\n",
    "                pred_pure_homo = rect_pred_flow(UV1, H12_)\n",
    "                preds = [pred_pure_homo, pred_UV2_opt, pred_UV2_raft, pred_UV2_raft_ft, pred_UV2_flowf, pred_UV2_opt_rect_algn,\n",
    "                        pred_UV2_raft_rect_algn, pred_UV2_raft_ft_rect_algn, pred_UV2_flowf_rect_algn]\n",
    "                errs_i = compute_errs(preds, UV2)\n",
    "                apt.append(errs_i)\n",
    "    metxs.append(apt)\n",
    "plot_progress3(metxs,tit=\"Average Pixel Error Across Difficulty Levels for KLB Bridge\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
